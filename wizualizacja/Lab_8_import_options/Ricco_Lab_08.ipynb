{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - import/eksport danych\n",
    "\n",
    "## Bezpośredni zapis do pliku\n",
    "\n",
    "Każdą strukturę danych (i nie tylko) utworzoną w Pythonie możemy zrzucić do pliku za pomocą funkcji biblioteki `pickle`. Piklowanie konwertuje dane do ciągu znaków (serializacja), z których skrypt Pythona jest w stanie bezpośrednio je odtworzyć, bez potrzeby konwersji. Jeżeli mamy strukturę `Series` lub `DataFrame`, to funkcja piklowania jest następująca: `struktura.to_pickle('ścieżka dostępu', compression='infer', protocol=4)`, gdzie:\n",
    "\n",
    "- `'ścieżka_dostępu'` - ścieżka, pod którą chcemy zapisać plik wraz z nazwą pliku. Jeżeli zostawiamy domyślną opcję `compression='infer'` a chcemy od razu skompresować plik, to podajemy odpowiednie rozszerzenie: ` .gzip, .bz2, .zip, .xz`. Jeżeli podamy inne rozszerzenie pliku (popularne opcje ` .txt, .pickle`) lub nie podamy rozszerzenia w ogóle (Windows wymaga podania rozszerzenia), to plik nie zostanie skompresowany.\n",
    "- `compression` - typ kompresji użytej przy tworzeniu pliku. Do wyboru:\n",
    "    - `'infer'` (domyślny) - typ kompresji (o ile w ogóle ma być użyta) zostanie dobrany po rozszerzeniu pliku,\n",
    "    - `‘gzip’, ‘bz2’, ‘zip’, ‘xz’` - zadany typ kompresji,\n",
    "    - `None` - bez kompresji.\n",
    "- `protocol` (domyślny: najwyższy dostępny) - typ protokołu użytego do serializacji danych. Do wyboru: `0,1,2,3,4`. Wersja 4 działa od Pythona 3.4, wersja 3 od 3.0, wersja 2 od 2.3, 1 i 0 działają zawsze. Im wyższa wersja, tym efektywniejsza serializacja (możliwość piklowania dużych obiektów, optymalizacja ze względu na typ danych itp.). Jeżeli podamy liczbę ujemną, użyty zostanie najwyższy dostępny protokół.\n",
    "\n",
    "Funkcja odczytu zapiklowanej struktury danych biblioteki Pandas to: `pandas.read_pickle('ścieżka dostępu', compression='infer')`.\n",
    "\n",
    "Przy domyślnych ustawieniach (`compression='infer'`) jeżeli plik był skompresowany, to zostanie rozpakowany metodą dobraną na podstawie jego rozszerzenia. Jeżeli plik nie posiada rozszerzenia a wiemy, że był skompresowany konkretną metodą, to zadajemy metodę `‘gzip’, ‘bz2’, ‘zip’` lub `‘xz’`. Jeżeli plik nie był skompresowany a posiada rozszerzenie sugerujące kompresję (jest to możliwe, np. gdy plik został zapiklowany z rozszerzeniem `.zip` przy ustawieniu `compression=None`, ale generalnie tak się nie robi), to należy podać `compression=None`.\n",
    "\n",
    "Typ odczytanej struktury będzie dokładnie taki, jak przed zapiklowaniem, tzn. jeżeli zapiklowaliśmy szereg `Series`, to po odczytaniu otrzymamy typ `Series`. Jeżeli chcemy sprawdzić typ zwróconej struktury, to używamy funkcji `type()`. Nie jest dobrym pomysłem wypisywanie na ekran danych nieznanego rozmiaru i typu.\n",
    "\n",
    "Przykład - piklowanie szeregu i ramki danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "szereg=pd.Series(range(0,100),index=range(1,101))\n",
    "ramka=pd.DataFrame({'kol1':{'w1':'a','w2':'b','w3':'c'},'kol2':{'w1':1,'w2':2,'w3':3}})\n",
    "\n",
    "szereg.to_pickle('dane01.zip')\n",
    "ramka.to_pickle('dane02.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odczyt pikli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1=pd.read_pickle('dane01.zip')\n",
    "data2=pd.read_pickle('dane02.zip')\n",
    "    \n",
    "print(type(data1)) \n",
    "print(type(data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/eksport z pliku csv\n",
    "\n",
    "Biblioteka Pandas posiada wbudowane funkcje do importu/eksportu danych w różnych formatach do odpowiednich struktur (`Series`, `DataFrame`). Obsługiwane typy:\n",
    "\n",
    "- pliki tekstowe typu `coma-separated values` (` .csv`),\n",
    "- pliki tekstowe typu `fixed-width` (dane w każdym wierzu ustalonej kolumny mają dokładnie taką samą liczbę znaków),\n",
    "- arkusze kalkulacyjne Excel (` .xls, .xlsx`),\n",
    "- bazy danych SQL (` .sql`),\n",
    "- dane w formacie JSON (` .json`), HTML (` .html`), HDF4 i HDF5 (` .hdf, .hdf5, .h5, .hdf4, .h4`),\n",
    "- pliki binarne w formacie Feather (` .feather`),\n",
    "- dane w formacie Apache Parquet (` .parquet`),\n",
    "- dane ładowane bezpośrednio z Google BigQuery,\n",
    "- pliki wyjściowe programów SAS (` .sas`) i STATA (` .ado, .gph`).\n",
    "\n",
    "Do importu struktury danych z pliku csv służy funkcja\n",
    "\n",
    "`pandas.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)`\n",
    "\n",
    "Opcje:\n",
    "- `filepath_or_buffer` - ścieżka dostępu do pliku, lub adres URL, np. `'file://localhost/path/to/table.csv'`,\n",
    "- `sep`, `delimiter` - string, domyślnie `','`; separator kolumn użyty w importowanym pliku. Jeżeli separator w pliku jest inny (np. tab lub spacja), to Python potrafi to wykryć automatycznie (przed zrzuceniem do struktury danych Python parsuje plik csv funkcją `csv.Sniffer`), dlatego spokojnie można pozostawić tę opcję domyślną przy ustawieniu `engine='python'`.\n",
    "- `header` - `None`, `'infer'`, liczba całkowita lub ich lista, domyślnie `'infer'`; w przypadku pojedynczej liczby jest to numer wiersza pliku csv, w którym znajdują się nazwy kolumn. Zrzucanie pliku do struktury danych rozpocznie się od tego wiersza - poprzednie zostaną zignorowane. Uwaga: `header=0` oznacza, że pierwszy wiersz pliku zawiera nazwy kolumn. Jeżeli w pliku nie ma nazw kolumn w ogóle, należy zadać `header=None`. `header='infer'` wykrywa tę różnicę automatycznie i ustawia albo 0, albo `None`. W przypadku danych multiindeksowanych zadajemy listę numerów wierszy, w których znajdują się indeksy w kolejności od ogólnych do szczególnych.\n",
    "- `names` - lista, tablica jednowymiarowa; lista nowych nazw kolumn (można też przy okazji dodać puste kolumny). Jeżeli plik nie zawierał nazw kolumn a chcemy je nadać używając tej opcji, to powinniśmy ustawić `header=None` zamiast opcji domyślnej. Jeżeli chcemy nadpisać istniejące nazwy, to powinniśmy ustawić `header=n`, gdzie `n` to numer wiersza zawierającego aktualne nazwy kolumn w pliku.\n",
    "- `index_col` - liczba, lista liczb lub `False`; numer kolumny, w której znajdują się indeksy wierszy (lub lista takich kolumn w przypadku danych multiindeksowanych), jeżeli plik taką kolumnę zawiera.\n",
    "- `usecols` - lista, macierz jednowymiarowa; liczby porządkowe lub nazwy kolumn, które chcemy zaimportować do struktury (jeżeli plik takie nazwy kolumn posiada lub nadaliśmy je opcją `names`)\n",
    "- `squeeze=True/False` - jeżeli `True` a plik zawiera tylko jedną kolumnę, to dane zostaną zrzucone do szeregu `Series`.\n",
    "- `prefix` - string; prefiks dodawany do automatycznego numeru kolumny w przypadku braku headera w pliku.\n",
    "- `mangle_dupe_cols=True/False` - jeżeli `True`, to duplikaty kolumn (kolumny o identycznych nazwach) zostaną rozróżnione przy zrzucaniu do struktury. Jeżeli `False`, to dane w kolumnach o zduplikowanych nazwach zostaną nadpisane (opcja dostępna wyłącznie w najnowszych dystrybucjach Pythona).\n",
    "- `dtype` - słownik; wymuszenie typów danych w kolumnach. Klucze to nazwy kolumn lub ich liczby porządkowe.\n",
    "- `engine` - `'c'` lub `'python'`; wybór parsera. Parser C jest szybszy, ale w przypadku pliku csv z nietypowymi separatorami lub innymi problemami polecam parser Pythona.\n",
    "- `converters` - słownik funkcji konwertujących wartości w poszczególnych kolumnach. Klucze to nazwy kolumn lub ich liczby porządkowe.\n",
    "- `true_values` - lista wartości, które mają być rozumiane jako `True`\n",
    "- `false_values` - lista wartości, które mają być rozumiane jako `False`\n",
    "- `skipinitialspace=True/False` - często w plikach csv po przecinku oddzielającym kolumny następuje spacja. Ustawienie `True` spowoduje jej pominięcie przy zrzucaniu danych do kolumn (dane nie będą w postaci `' tekst'`, tylko `'tekst'`.\n",
    "- `skiprows` - lista, liczba lub funkcja; lista numerów wierszy lub liczba wierszy (licząc od góry), które zostaną pominięte przy imporcie danych. Można również wywołać funkcję (filtr) zwracającą `True/False`. Jeżeli funkcja zwróci `True`, to wiersz zostanie pominięty.\n",
    "- `skipfooter` - liczba wierszy, które zostaną pominięte, licząc od dołu pliku.\n",
    "- `nrows` - liczba; liczba wierszy, które zostaną zaimportowane (funkcja przydatna przy zrzucaniu pliku po kawałku).\n",
    "- `na_values` - lista stringów lub słownik; lista wartości, które mają być rozumiane jako brak danej `NaN`. W przypadku podania słownika, w którym kluczami są nazwy kolumn możemy ustawić osobne listy dla każdej kolumny.\n",
    "- `keep_default_na=True/False` - jeżeli `True`, to zawsze stringi `‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘n/a’, ‘nan’, ‘null’` będą rozumiane jako `NaN`.\n",
    "- `na_filter=True/False` - automatyczne wykrywanie braku danych `NaN` (domyślne i z listy `na_values`).\n",
    "- `verbose=True/False` - jeżeli `True` w miejscu brakujących danych zostanie wypisane explicite `NaN`.\n",
    "- `skip_blank_lines=True/False` - jeżeli `True` puste wiersze zostaną pominięte zamiast wypełnienia `NaN`.\n",
    "- `parse_dates` - `True/False`, lista kolumn (po nazwach lub indeksach), lista list albo słownik. Automatyczne wykrywanie daty:\n",
    "    - `True` - parser wyszuka daty w nazwach wierszy i kolumn,\n",
    "    - lista `[...]` indeksów kolumn - elementy tych kolumn zostaną potraktowane jako daty,\n",
    "    - lista list `[[...]]` indeksów kolumn - kolumny podane w ,,podwójnej'' liście zostaną połączone w jedną kolumnę daty (użyteczne w przypadku, gdy w oryginalnych danych jedna kolumna to dzień, druga miesiąc a trzecia to rok),\n",
    "    - słownik `{'nazwa':[...]}` dane z kolumn z listy zostaną połączone w datę i powstała kolumna zostanie nazwana `nazwa`.\n",
    "- `infer_datetime_format=True/False` - format daty zgodny z podanym w pliku (o ile się da) plus szybsze parsowanie daty,\n",
    "- `keep_date_col=True/False` - jeżeli `True`, to przy parsowaniu daty z kilku kolumn w jedną zachowane zostaną oryginalne kolumny,\n",
    "- `date_parser` - parser przekształcający tekst w dane. Domyślnie `dateutil.parser.parser`.\n",
    "- `dayfirst=True/False` - włącza/wyłącza format daty DD/MM,\n",
    "- `compression` - automatyczne rozpakowywanie pliku przed importem; do wyboru `‘infer’, ‘gzip’, ‘bz2’, ‘zip’, ‘xz’, None`,\n",
    "- `thousands` - string; postać separatora tysięcznego w importowanych danych,\n",
    "- `decimal` - string; postać separatora dziesiętnego w importowanych danych; domyślnie `'.'`,\n",
    "- `lineterminator` - znak końca linii w importowanych danych; działa wyłącznie z parserem C,\n",
    "- `quotechar` - znak początku/końca cytatu (wszelkie znaki formatujące wewnątrz cytatu zostaną zignorowane)\n",
    "- `comment` - znak początku komentarza w importowanym pliku. Zawartość linii po tym znaku zostanie zignorowana.\n",
    "- `encoding` - typ kodowania przy odczycie pliku (domyślnie `'utf-8'`),\n",
    "- `dialect` - możliwość wyboru jednego ze standardów znaków specjalnych (separator kolumn, cytat itp.) z biblioteki `csv.Dialect`,\n",
    "- `error_bad_lines=True/False` -  jeżeli `True`, to w przypadku pliku csv z błędnymi wierszami (więcej separatorów kolumnowych niż kolumn) plik nie zostanie zaimportowany do struktury i wyświetli się komunikat błędu. Jeżeli `False`, to błędne wiersze zostaną pominięte i struktura danych zostanie utworzona.\n",
    "- `warn_bad_lines=True/False` - jeżeli `True` oraz `error_bad_lines=False`, to pojawi się ostrzeżenie o opuszczonych błędnych wierszach.\n",
    "- `delim_whitespace=True/False` - jeżeli `True`, to spacje i taby będą uważane za separator kolumnowy,\n",
    "- `low_memory=True/False` - jeżeli `True`, oszczędza pamięć w trakcie parsowania parserem C,\n",
    "- `memory_map=True/False` - jeżeli podana jest ścieżka dostępu do pliku (nie URL), to dane są zczytywane bezpośrednio z pamięci, z pominięciem klasycznej ścieżki odczytu/zapisu pliku (zaawansowana opcja obsługi olbrzymich zbiorów danych),\n",
    "- `float_precision` - string `None, 'high', 'round_trip'`; wybór rodzaju konwertera liczb zmiennoprzecinkowych dla parsera C. `None` to domyślny konwerter. Wybór zależy od liczby miejsc po przecinku, jakie chcemy osiągnąć."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do eksportu danych ze struktury danych (`DataFrame` lub `Series`) do pliku csv używamy funkcji `struktura.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression='infer', quoting=None, quotechar='\"', line_terminator=None, chunksize=None, date_format=None, doublequote=True, escapechar=None, decimal='.')`, gdzie:\n",
    "\n",
    "- `path_or_buf` - ścieżka dostępu,\n",
    "- `sep, decimal, quotechar` - jak wyżej,\n",
    "- `na_rep` - string; reprezentacja braku danych `NaN`,\n",
    "- `float_format` - string formatujący zmienne zmiennoprzecinkowe,\n",
    "- `columns` - lista kolumn do eksportu,\n",
    "- `header` - `True/False` lub lista stringów; spowoduje zapis nazw kolumn w pierwszej linii pliku csv,\n",
    "- `index=True/False` - `True` spowoduje zapis do csv kolumny zawierającej indeksy wierszy,\n",
    "- `index_label` - opcjonalna nazwa dla kolumny indeksów wierszy,\n",
    "- `mode` - tryb zapisu do pliku, domyślnie `'w'`,\n",
    "- `encoding` - wybór kodowania,\n",
    "- `compression` - wybór rodzaju kompresji (jak wyżej),\n",
    "- `line_terminator` - string; wybór znaku końca linii - domyślnie `os.linesep`, czyli znak odpowiedni dla systemu operacyjnego,\n",
    "- `chunksize` - liczba lub `None`; liczba wierszy eksportowanych \"na raz\" (opcja dla dużych zbiorów danych),\n",
    "- `date_format` - string formatujący dla obiektów typu `datetime`, czyli daty i godziny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/eksport z arkusza kalkulacyjnego\n",
    "\n",
    "Odczyt danych z arkusza kalkulacyjnego najczęściej jest dużo mniej kłopotliwy, niż z pliku csv. Używamy do tego funkcji `pandas.read_excel(io, sheet_name=0, header=0, names=None, parse_cols=None, usecols=None, squeeze=False, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skiprows=None, nrows=None, na_values=None, keep_default_na=True, verbose=False, parse_dates=False, date_parser=None, thousands=None, comment=None, skipfooter=0, convert_float=True, mangle_dupe_cols=True)`, gdzie\n",
    "\n",
    "- `io` - string; ścieżka dostępu do pliku Excela lub URL,\n",
    "- `sheet_name` - string, liczba lub lista; nazwa (nazwy) lub liczba (liczby) porządkowe arkuszy w pliku, które chcemy zaimportować. Domyślnie 0 (pierwszy arkusz). W przypadku podania większej liczby arkuszy otrzymamy słownik ramek danych.\n",
    "- `header` - liczba lub lista liczb; numer wiersza zawierającego indeksy kolumn (jak przy imporcie csv)\n",
    "- `names` - lista nazw kolumn (jak przy imporcie z csv),\n",
    "- `usecols` - lista nazw lub indeksów kolumn do zaimportowania,\n",
    "- `squeeze=True/False` - jeżeli arkusz zawiera tylko jedną kolumnę, to zwróci szereg `Series`,\n",
    "- `dtype` - lista lub słownik wymuszonych typów danych dla kolumn,\n",
    "- `engine` - do wyboru: `None, 'xldrl'`,\n",
    "- `converters` - słownik konwerterów typów danych dla poszczególnych kolumn,\n",
    "- `true_values, false_values, na_values, keep_default_na, verbose` - jak przy imporcie csv,\n",
    "- `skiprows` - jak przy imporcie csv,\n",
    "- `nrows` - liczba wierszy parsowanych na raz,\n",
    "- `parse_dates, date_parser` - jak przy imporcie csv,\n",
    "- `thousands` - separator tysięczny; używany wyłącznie przy imporcie danych tekstowych jako liczb (dane liczbowe zostaną automatycznie przekonwertowane na format Pythona),\n",
    "- `comment` - jak przy imporcie csv,\n",
    "- `skipfooter` - liczba wierszy opuszczanych przy imporcie licząc od dołu,\n",
    "- `convert_float=True/False` - konwertuje liczby całkowite zapisane w formacie zmiennoprzecinkowym na klasyczny `int`,\n",
    "- `mangle_dupe_cols=True/False` - jak przy imporcie csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do eksportu struktury danych (`DataFrame`, `Series`) do arkusza Excela służy funkcja `struktura.to_excel(excel_writer, sheet_name='Sheet1', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, inf_rep='inf', verbose=True, freeze_panes=None)`, gdzie:\n",
    "\n",
    "- `excel_writer` - ścieżka dostępu,\n",
    "- `sheet_name` - string; nazwa utworzonego arkusza,\n",
    "- `na_rep, float_format` - jak przy eksporcie do csv,\n",
    "- `columns` - lista kolumn do eksportu,\n",
    "- `header, index, index_label` - jak przy eksporcie do csv,\n",
    "- `startrow` - wiersz lewej górnej komórki w arkuszu, od której mają się zaczynać eksportowane dane,\n",
    "- `startcol` - kolumna lewej górnej komórki w arkuszu, od której mają się zaczynać eksportowane dane,\n",
    "- `engine` - do wyboru `‘openpyxl’, ‘xlsxwriter’`,\n",
    "- `merge_cells=True/False` - eksport ramki multiindeksowanej od razu z odpowiednim połączeniem komórek,\n",
    "- `encoding` - wybór kodowania,\n",
    "- `inf_rep` - string; reprezentacja nieskończoności (domyślnie `'inf'`),\n",
    "- `verbose=True/False` - bardziej szczegółowy raport ewentualnych błędów przy eksporcie,\n",
    "- `freeze_panes` - para liczb; położenie najniższego wiersza i najdalszej kolumny, które mają być zamrożone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1.\n",
    "Zaimportować plik `good.csv` oraz `good1.csv` do struktur danych wyglądających następująco:\n",
    "```\n",
    "             klienci    obrót     zysk\n",
    "data                                 \n",
    "2019-06-05       20  1034.32   245.93\n",
    "2019-07-05       32  2054.43   543.23\n",
    "2019-08-05       19  1042.56   234.99\n",
    "2019-09-05       24  2450.42   482.24\n",
    "2019-10-05       42  3010.86   732.13\n",
    "2019-11-05       53  4529.34  1009.54\n",
    "2019-05-13       25  1358.60   358.64\n",
    "2019-05-14       19   934.45   289.13\n",
    "2019-05-15       36  2874.84   523.49\n",
    "2019-05-16       22  1873.12   346.97\n",
    "2019-05-17       36  3010.11   704.21\n",
    "2019-05-18       48  3722.13   989.06\n",
    "```\n",
    "\n",
    "Zapiklować jedną otrzymaną strukturę do pliku `mydlo_powidlo.zip` a drugą wyeksportować do arkusza Excel, plik `mydlo_powidlo.xlsx`, nazwa arkusza `Obroty maj`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            klienci    obrót     zysk\n",
      "data                                 \n",
      "2019-05-06       20  1034.32   245.93\n",
      "2019-05-07       32  2054.43   543.23\n",
      "2019-05-08       19  1042.56   234.99\n",
      "2019-05-09       24  2450.42   482.24\n",
      "2019-05-10       42  3010.86   732.13\n",
      "2019-05-11       53  4529.34  1009.54\n",
      "2019-05-13       25  1358.60   358.64\n",
      "2019-05-14       19   934.45   289.13\n",
      "2019-05-15       36  2874.84   523.49\n",
      "2019-05-16       22  1873.12   346.97\n",
      "2019-05-17       36  3010.11   704.21\n",
      "2019-05-18       48  3722.13   989.06\n",
      "            klienci    obrót     zysk\n",
      "data                                 \n",
      "2019-05-06       20  1034.32   245.93\n",
      "2019-05-07       32  2054.43   543.23\n",
      "2019-05-08       19  1042.56   234.99\n",
      "2019-05-09       24  2450.42   482.24\n",
      "2019-05-10       42  3010.86   732.13\n",
      "2019-05-11       53  4529.34  1009.54\n",
      "2019-05-13       25  1358.60   358.64\n",
      "2019-05-14       19   934.45   289.13\n",
      "2019-05-15       36  2874.84   523.49\n",
      "2019-05-16       22  1873.12   346.97\n",
      "2019-05-17       36  3010.11   704.21\n",
      "2019-05-18       48  3722.13   989.06\n"
     ]
    }
   ],
   "source": [
    "good=pd.read_csv('good.csv',parse_dates={'data':[0,1,2]},dayfirst=True,index_col='data')\n",
    "good1=pd.read_csv('good1.csv',parse_dates={'data':[0,1,2]},dayfirst=True,index_col='data',lineterminator=\";\")\n",
    "print(good)\n",
    "print(good1)\n",
    "\n",
    "good.to_pickle('mydlo_powidlo.zip')\n",
    "good1.to_excel('mydlo_powidlo.xlsx',sheet_name='Obroty maj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2.\n",
    "Zaimportować plik `bad.csv` i plik `worse.csv` do struktur danych wyglądających następująco:\n",
    "```\n",
    "\t      klienci \"M,P\"  obrót \"M,P\"  zysk \"M,P\"\n",
    "data                                              \n",
    "2019-06-05             20      1034.32      245.93\n",
    "2019-07-05             32      2054.43      543.23\n",
    "2019-08-05             19      1042.56      234.99\n",
    "2019-09-05             24      2450.42      482.24\n",
    "2019-10-05             42      3010.86      732.13\n",
    "2019-11-05             53      4529.34     1009.54\n",
    "2019-05-13             25      1358.60      358.64\n",
    "2019-05-14             19       934.45      289.13\n",
    "2019-05-15             36      2874.84      523.49\n",
    "2019-05-16             22      1873.12      346.97\n",
    "2019-05-17             36      3010.11      704.21\n",
    "2019-05-18             48      4722.13     1289.06\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>klienci \"M,P\"</th>\n",
       "      <th>obrót \"M,P\"</th>\n",
       "      <th>zysk \"M,P\"</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>20</td>\n",
       "      <td>1034.32</td>\n",
       "      <td>245.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>32</td>\n",
       "      <td>2054.43</td>\n",
       "      <td>543.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>19</td>\n",
       "      <td>1042.56</td>\n",
       "      <td>234.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>24</td>\n",
       "      <td>2450.42</td>\n",
       "      <td>482.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>42</td>\n",
       "      <td>3010.86</td>\n",
       "      <td>732.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>53</td>\n",
       "      <td>4529.34</td>\n",
       "      <td>1009.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>25</td>\n",
       "      <td>1358.60</td>\n",
       "      <td>358.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>19</td>\n",
       "      <td>934.45</td>\n",
       "      <td>289.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>36</td>\n",
       "      <td>2874.84</td>\n",
       "      <td>523.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>22</td>\n",
       "      <td>1873.12</td>\n",
       "      <td>346.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>36</td>\n",
       "      <td>3010.11</td>\n",
       "      <td>704.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>48</td>\n",
       "      <td>4722.13</td>\n",
       "      <td>1289.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            klienci \"M,P\"  obrót \"M,P\"  zysk \"M,P\"\n",
       "data                                              \n",
       "2019-05-06             20      1034.32      245.93\n",
       "2019-05-07             32      2054.43      543.23\n",
       "2019-05-08             19      1042.56      234.99\n",
       "2019-05-09             24      2450.42      482.24\n",
       "2019-05-10             42      3010.86      732.13\n",
       "2019-05-11             53      4529.34     1009.54\n",
       "2019-05-13             25      1358.60      358.64\n",
       "2019-05-14             19       934.45      289.13\n",
       "2019-05-15             36      2874.84      523.49\n",
       "2019-05-16             22      1873.12      346.97\n",
       "2019-05-17             36      3010.11      704.21\n",
       "2019-05-18             48      4722.13     1289.06"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad=pd.read_csv('bad.csv',header=1 ,parse_dates={'data':[0,1,2]},dayfirst=True,usecols=[0,1,2,3,4,5],index_col='data',names=['d1','d2','d3','klienci \"M,P\"','obrót \"M,P\"','zysk \"M,P\"'],comment='%')\n",
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10202/463445861.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  worse=pd.read_csv('worse.csv',sep=\"-\",error_bad_lines=False,header=1 ,parse_dates={'data':[0,1,2]},dayfirst=True,index_col='data',names=['d1','d2','d3','klienci \"M,P\"','obrót \"M,P\"','zysk \"M,P\"'],comment='%',decimal=\",\",thousands=\" \")\n",
      "Skipping line 10: expected 6 fields, saw 9\n",
      "Skipping line 14: expected 6 fields, saw 10\n",
      "Skipping line 16: expected 6 fields, saw 13\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>klienci \"M,P\"</th>\n",
       "      <th>obrót \"M,P\"</th>\n",
       "      <th>zysk \"M,P\"</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-06</th>\n",
       "      <td>20</td>\n",
       "      <td>1034.32</td>\n",
       "      <td>245.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07</th>\n",
       "      <td>32</td>\n",
       "      <td>2054.43</td>\n",
       "      <td>543.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08</th>\n",
       "      <td>19</td>\n",
       "      <td>1042.56</td>\n",
       "      <td>234.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-09</th>\n",
       "      <td>24</td>\n",
       "      <td>2450.42</td>\n",
       "      <td>482.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-10</th>\n",
       "      <td>42</td>\n",
       "      <td>3010.86</td>\n",
       "      <td>732.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-11</th>\n",
       "      <td>53</td>\n",
       "      <td>4529.34</td>\n",
       "      <td>1009.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-13</th>\n",
       "      <td>25</td>\n",
       "      <td>1358.60</td>\n",
       "      <td>358.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-14</th>\n",
       "      <td>19</td>\n",
       "      <td>934.45</td>\n",
       "      <td>289.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-15</th>\n",
       "      <td>36</td>\n",
       "      <td>2874.84</td>\n",
       "      <td>523.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-16</th>\n",
       "      <td>22</td>\n",
       "      <td>1873.12</td>\n",
       "      <td>346.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>36</td>\n",
       "      <td>3010.11</td>\n",
       "      <td>704.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-18</th>\n",
       "      <td>48</td>\n",
       "      <td>4722.13</td>\n",
       "      <td>1289.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            klienci \"M,P\"  obrót \"M,P\"  zysk \"M,P\"\n",
       "data                                              \n",
       "2019-05-06             20      1034.32      245.93\n",
       "2019-05-07             32      2054.43      543.23\n",
       "2019-05-08             19      1042.56      234.99\n",
       "2019-05-09             24      2450.42      482.24\n",
       "2019-05-10             42      3010.86      732.13\n",
       "2019-05-11             53      4529.34     1009.54\n",
       "2019-05-13             25      1358.60      358.64\n",
       "2019-05-14             19       934.45      289.13\n",
       "2019-05-15             36      2874.84      523.49\n",
       "2019-05-16             22      1873.12      346.97\n",
       "2019-05-17             36      3010.11      704.21\n",
       "2019-05-18             48      4722.13     1289.06"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worse=pd.read_csv('worse.csv',sep=\"-\",error_bad_lines=False,header=1 ,parse_dates={'data':[0,1,2]},dayfirst=True,index_col='data',names=['d1','d2','d3','klienci \"M,P\"','obrót \"M,P\"','zysk \"M,P\"'],comment='%',decimal=\",\",thousands=\" \")\n",
    "worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3.\n",
    "Zaimportować plik `disaster.csv` do struktury wyglądającej jak powyżej. Gotową strukturę wyeksportować do pliku `mydlo_powidlo.csv`. Zaimportować ją z powrotem i sprawdzić typy zmiennych w kolumnach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             klienci \"M,P\"   obrót \"M,P\"   zysk \"M,P\"\n",
      "data                                                 \n",
      "2019-05-06              20       1034.32       245.93\n",
      "2019-05-07              32       2054.43       543.23\n",
      "2019-05-08              19       1042.56       234.99\n",
      "2019-05-09              24       2450.42       482.24\n",
      "2019-05-10              42       3010.86       732.13\n",
      "2019-05-11              53       4529.34      1009.54\n",
      "2019-05-13              25       1358.60       358.64\n",
      "2019-05-14              19        934.45       289.13\n",
      "2019-05-15              36       2874.84       523.49\n",
      "2019-05-16              22       1873.12       346.97\n",
      "2019-05-17              36       3010.11       704.21\n",
      "2019-05-18              48       4722.13      1289.06\n"
     ]
    }
   ],
   "source": [
    "disaster=pd.read_csv('disaster.csv',sep=\"&\", header=1,\n",
    "                     usecols=[0,1,2,4,6,7],\n",
    "                     parse_dates={'data':[0,1,2]},dayfirst=True,\n",
    "                     index_col='data',comment=\"/\",\n",
    "                     skiprows=[10,17,18],\n",
    "                    decimal=\",\",thousands=\" \")\n",
    "print(disaster)\n",
    "\n",
    "disaster.to_csv('mydlo_powidlo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data               object\n",
       " klienci \"M,P\"      int64\n",
       " obrót \"M,P\"      float64\n",
       " zysk \"M,P\"       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster= pd.read_csv('mydlo_powidlo.csv')\n",
    "disaster.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4.\n",
    "Zaimportować z pliku `grupa00.xlsx` obydwa arkusze:\n",
    "- każdy arkusz do osobnej struktury,\n",
    "- obydwa arkusze razem do słownika struktur.\n",
    "\n",
    "Odpowiednio wybrać kolumnę numerującą wiersze, aby uniknąć niepotrzebnej podwójnej numeracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    Imię    Nazwisko  kol1  kol2  kol3  suma\n",
       " lp                                          \n",
       " 1   Anna     Borówka   7.0   3.0   5.5  15.5\n",
       " 2    Jan      Brokuł   6.0   2.0   3.0  11.0\n",
       " 3    Jan      Cebula   4.5   2.0   7.5  14.0\n",
       " 4   Anna     Cukinia   2.0   4.6   3.5  10.1\n",
       " 5   Anna       Dynia   3.0   5.0   8.0  16.0\n",
       " 6    Jan      Jarmuż   4.5   2.5   6.5  13.5\n",
       " 7   Anna    Kalafior   6.0   6.0   3.0  15.0\n",
       " 8    Jan     Kapusta   8.5   3.5   2.5  14.5\n",
       " 9    Jan   Marchewka   3.0   8.0   4.5  15.5\n",
       " 10  Anna      Oliwka   3.5   7.5   6.0  17.0\n",
       " 11  Anna   Pasternak   0.0   3.5   2.0   5.5\n",
       " 12   Jan  Pietruszka   NaN   4.0   1.5   5.5\n",
       " 13  Anna         Por   6.0   2.5   1.0   9.5\n",
       " 14   Jan      Sałata   4.5   4.0   0.0   8.5\n",
       " 15   Jan     Tatarak   2.0   5.0   3.5  10.5\n",
       " 16  Anna    Ziemniak   3.0   5.5   7.0  15.5,\n",
       "     Imię    Nazwisko  kol1  kol2  kol3  suma\n",
       " lp                                          \n",
       " 1   Anna     Borówka   3.0   3.0   5.5  11.5\n",
       " 2    Jan      Brokuł   6.0   2.5   3.0  11.5\n",
       " 3    Jan      Cebula   4.5   1.5   7.5  13.5\n",
       " 4   Anna     Cukinia   2.0   4.6   3.5  10.1\n",
       " 5   Anna       Dynia   3.0   5.0   5.0  13.0\n",
       " 6    Jan      Jarmuż   4.5   2.5   6.5  13.5\n",
       " 7   Anna    Kalafior   3.0   6.0   6.5  15.5\n",
       " 8    Jan     Kapusta   8.5   3.5   2.5  14.5\n",
       " 9    Jan   Marchewka   5.5   8.0   4.5  18.0\n",
       " 10  Anna      Oliwka   3.5   7.5   6.0  17.0\n",
       " 11  Anna   Pasternak   4.0   3.5   1.0   8.5\n",
       " 12   Jan  Pietruszka   2.5   NaN   1.5   4.0\n",
       " 13  Anna         Por   6.0   2.5   1.0   9.5\n",
       " 14   Jan      Sałata   4.5   3.5   0.0   8.0\n",
       " 15   Jan     Tatarak   2.0   5.0   3.5  10.5\n",
       " 16  Anna    Ziemniak   2.0   7.0   6.5  15.5)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem1 = pd.read_excel('grupa00.xlsx','Semestr I',index_col=0)\n",
    "sem2 = pd.read_excel('grupa00.xlsx','Semestr II',index_col=0)\n",
    "sem1,sem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0:     Imię    Nazwisko  kol1  kol2  kol3  suma\n",
       " lp                                          \n",
       " 1   Anna     Borówka   7.0   3.0   5.5  15.5\n",
       " 2    Jan      Brokuł   6.0   2.0   3.0  11.0\n",
       " 3    Jan      Cebula   4.5   2.0   7.5  14.0\n",
       " 4   Anna     Cukinia   2.0   4.6   3.5  10.1\n",
       " 5   Anna       Dynia   3.0   5.0   8.0  16.0\n",
       " 6    Jan      Jarmuż   4.5   2.5   6.5  13.5\n",
       " 7   Anna    Kalafior   6.0   6.0   3.0  15.0\n",
       " 8    Jan     Kapusta   8.5   3.5   2.5  14.5\n",
       " 9    Jan   Marchewka   3.0   8.0   4.5  15.5\n",
       " 10  Anna      Oliwka   3.5   7.5   6.0  17.0\n",
       " 11  Anna   Pasternak   0.0   3.5   2.0   5.5\n",
       " 12   Jan  Pietruszka   NaN   4.0   1.5   5.5\n",
       " 13  Anna         Por   6.0   2.5   1.0   9.5\n",
       " 14   Jan      Sałata   4.5   4.0   0.0   8.5\n",
       " 15   Jan     Tatarak   2.0   5.0   3.5  10.5\n",
       " 16  Anna    Ziemniak   3.0   5.5   7.0  15.5,\n",
       " 1:     Imię    Nazwisko  kol1  kol2  kol3  suma\n",
       " lp                                          \n",
       " 1   Anna     Borówka   3.0   3.0   5.5  11.5\n",
       " 2    Jan      Brokuł   6.0   2.5   3.0  11.5\n",
       " 3    Jan      Cebula   4.5   1.5   7.5  13.5\n",
       " 4   Anna     Cukinia   2.0   4.6   3.5  10.1\n",
       " 5   Anna       Dynia   3.0   5.0   5.0  13.0\n",
       " 6    Jan      Jarmuż   4.5   2.5   6.5  13.5\n",
       " 7   Anna    Kalafior   3.0   6.0   6.5  15.5\n",
       " 8    Jan     Kapusta   8.5   3.5   2.5  14.5\n",
       " 9    Jan   Marchewka   5.5   8.0   4.5  18.0\n",
       " 10  Anna      Oliwka   3.5   7.5   6.0  17.0\n",
       " 11  Anna   Pasternak   4.0   3.5   1.0   8.5\n",
       " 12   Jan  Pietruszka   2.5   NaN   1.5   4.0\n",
       " 13  Anna         Por   6.0   2.5   1.0   9.5\n",
       " 14   Jan      Sałata   4.5   3.5   0.0   8.0\n",
       " 15   Jan     Tatarak   2.0   5.0   3.5  10.5\n",
       " 16  Anna    Ziemniak   2.0   7.0   6.5  15.5}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem1_2 = pd.read_excel('grupa00.xlsx',index_col=0,sheet_name=[0,1])\n",
    "sem1_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
